 You will generate a full endâ€‘toâ€‘end implementation based on these requirements:

---

## ğŸ§  Project Scope & Goals

Design and implement a **Jarvisâ€‘style, voiceâ€‘enabled multiâ€‘agent AI system** featuring:
- Voice input (live mic) and voice output (speakers) via open-source STT/TTS or ElevenLabs API
- Multiple agents using different LLMs (OpenRouter & Ollama)
- Realâ€‘time WebSocket frontend for live streaming voice/text
- Persistent memory and audit logs in PostgreSQL
- Vector retrieval and RAG using Qdrant
- File ingestion and storage for RAG
- Dynamic tool installation via Smithery MCP registry and AutoGenâ€™s `McpWorkbench`
- Reflexionâ€‘based selfâ€‘improvement loops
- Cost tracking and budget control in DB
- Containerized via Docker Compose
- Extensible for future RL pipeline integration

---

## ğŸ”‰ Voice Control

- Input: live microphone audio via open-source STT (e.g., WhisperX, Vocode) or optional ElevenLabs / AssemblyAI ASR :contentReference[oaicite:1]{index=1}  
- Output: realâ€‘time TTS through open models (Coqui, pyttsx3, Dia) or ElevenLabs :contentReference[oaicite:2]{index=2}  
- Alternative: use Pipecat framework for full voiceâ€‘pipeline orchestration :contentReference[oaicite:3]{index=3}

---

## ğŸ‘¥ Agent LLM Configurations

Agents initialized with distinct LLMs:

```python
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.ollama import OllamaChatCompletionClient

agent_clients = {
  "agent1_openrouter_gpt40": OpenAIChatCompletionClient(
      model="gpt-4o",
      base_url="https://openrouter.ai/api/v1",
      api_key=env("OPENROUTER_API_KEY")
  ),
  "agent2_ollama_gemma3_7b": OllamaChatCompletionClient(model="gemma3-7b"),
  "agent3_openrouter_gemini25": OpenAIChatCompletionClient(
      model="gemini-2.5-flash",
      base_url="https://openrouter.ai/api/v1",
      api_key=env("OPENROUTER_API_KEY")
  ),
  "agent4_ollama_llama4_32b": OllamaChatCompletionClient(model="llama4-32b")
}
Agents have independent model_clients and can be swapped as needed.

All agents have access to shared infra: PostgreSQL, pgvector, Qdrant, /data/docs, MCP tools via Smithery and McpWorkbench.

ğŸ—ï¸ Infrastructure Components
PostgreSQL + pgvector â€“ Tables: chat_logs, tool_registry, cost_history, file_index, reflexion_log

Qdrant â€“ Vector store for RAG and retrieval

File volume â€“ /data/docs for persistent file storage

Ollama Daemon â€“ Docker service for gemma3â€‘7b & llama4â€‘32b

Agent Service â€“ Python process handling Autogen orchestration, MCP, Reflexion, voice

FastAPI WebSocket API â€“ Streams WebSocket for voice/text interactions

Docker Compose â€“ Brings all services together

Smithery MCP CLI â€“ For tool discovery, install, plus MCPSafetyScanner validation

ğŸ—‚ï¸ ManagerAgent Workflow
Task reception via UserProxyAgent (WebSocket)

STT â€“ Convert mic audio to text

Task planning â€“ Delegate to appropriate agent(s)

MCP tool check â€“ Query McpWorkbench.list_tools(), if missing â†’ smithery install, validate via MCPSafetyScanner, reload, log in DB

Tool execution â€“ Call tool, stream tool outputs, log cost to cost_history

Memory & RAG â€“ Log chat/tools/output, embed and upsert into Qdrant, save files to /data/docs

LLM response â€“ Return response via agent, synthesize text to speech, stream back over WebSocket

Reflexion â€“ Analyze task success/failure, extract heuristics, store in reflexion_log

Summary & cleanup â€“ Provide final summary, cost and vector index report

ğŸ§© Constraints & Best Practices
Cost management â€“ Budget LLM usage and log all consumption

Turn limit â€“ max_turns=15 per user session

Security â€“ Validate all MCP installs before use

Resilience â€“ Robust retries and error handling

LLM swappability â€“ Agents must support runtime model_client changes

Streaming â€“ Real-time voice/text streaming via WebSocket

ğŸ“ Required Output Files
Generate these components with production-grade quality:

docker-compose.yml â€“ Services: postgres+pgvector, qdrant, ollama, agent_service, fastapi_ws, voice_adapter

service/agent.py â€“ Autogen orchestrator with agents, MCP, voice, memory, reflexion

service/db_schema.sql â€“ PostgreSQL schema and indexes

service/retrieval.py â€“ Qdrant vector and RAG integration

service/frontend.py â€“ FastAPI WebSocket + voice pipeline

service/voice.py â€“ STT/TTS adapters (WhisperX, Coqui, ElevenLabs)

service/mcp_integration.py â€“ Smithery, McpWorkbench, MCPSafetyScanner utility

.env.example â€“ Template for API keys and DB URLs

This prompt covers architecture, voice integration, tooling, DB design, and agents. Generate each file as clean, cohesive code following these instructions.